{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CranfieldCorpus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragmalingu/experiments/blob/master/00_Data/CranfieldCorpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOrLXNr5DVER"
      },
      "source": [
        "# Setup an Elasticsearch Instance in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5AzHO1FDVEU"
      },
      "source": [
        "Everthing to connect to Elasticsearch, for detailed explaination see [this Notebook.](https://)\n",
        "Download:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBnu7OFEDVEX"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "# download elasticsearch\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.1-linux-x86_64.tar.gz -q\n",
        "!tar -xzf elasticsearch-7.9.1-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.9.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxgS1p4_-zun"
      },
      "source": [
        "Start a local server:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8rAj0Ti-pOC"
      },
      "source": [
        "# start server\n",
        "es_server = Popen(['elasticsearch-7.9.1/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                 )\n",
        "# client-side\n",
        "!pip install elasticsearch -q\n",
        "from elasticsearch import Elasticsearch\n",
        "from datetime import datetime\n",
        "es = Elasticsearch()\n",
        "es.ping()  # got True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xRvatFztoo"
      },
      "source": [
        "# Parsing Cranfield Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fS6lVgXAO9F"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).  <br>\n",
        "For detailed information about the format of the files, see the PragmaLingu [Data Sets](https://pragmalingu.de/docs/guides/data-comparison). \n",
        "You can learn about parsing in general by reading our [parsing guide](https://pragmalingu.de/docs/guides/how-to-parse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUv41dfn9nzz"
      },
      "source": [
        "### Dowlnoad Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEh_A4MBj5Ii"
      },
      "source": [
        "Download and unzip data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsE0BqpwjrI1"
      },
      "source": [
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
        "!tar -xf cran.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuLhLXSXCOCb"
      },
      "source": [
        "Set paths to the dowloaded data as variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNdrPjLX_IQO"
      },
      "source": [
        "PATH_TO_CRAN_TXT = '/content/cran.all.1400'\n",
        "PATH_TO_CRAN_QRY = '/content/cran.qry'\n",
        "PATH_TO_CRAN_REL = '/content/cranqrel'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUbNSPRGI97k"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9xL0ECIHkRH"
      },
      "source": [
        "Make all the imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhkNZh0iHi5A"
      },
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPxu5qy4CQI9"
      },
      "source": [
        "### Process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDy0lP-_j4LL"
      },
      "source": [
        "Get the text entries from the text and query file preprocessed as a list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csJgew0Zjv_L"
      },
      "source": [
        "ID_marker = re.compile('^\\.I',re.MULTILINE)\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read()\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "cran_txt_list = get_data(PATH_TO_CRAN_TXT, ID_marker)\n",
        "cran_qry_list = get_data(PATH_TO_CRAN_QRY, ID_marker)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zZyhZorG6sE"
      },
      "source": [
        "Process the list of the text file into nested dictionaries which can be saved as json:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWWjrq4iku4n"
      },
      "source": [
        "cran_chunk_start = re.compile('\\.[A,B,T,W]')\n",
        "\n",
        "# process text file\n",
        "\n",
        "cran_chunk_start = re.compile('\\.[A,B,T,W]')\n",
        "cran_txt_data = defaultdict(dict)\n",
        "\n",
        "for line in cran_txt_list:\n",
        "  entries= re.split(cran_chunk_start,line)\n",
        "  id = entries[0].strip()\n",
        "  title = entries[1]\n",
        "  author = entries[2]\n",
        "  publication_date = entries[3]\n",
        "  text = entries[4:]\n",
        "  cran_txt_data[id]['title'] = ''.join(title)\n",
        "  cran_txt_data[id]['author'] = author\n",
        "  cran_txt_data[id]['publication_date'] = publication_date\n",
        "  cran_txt_data[id]['text'] = ''.join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7vBdNBIIJQg"
      },
      "source": [
        "Same process as the text files with the query file but with less information to parse:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW7FYxoiH7ab"
      },
      "source": [
        "qry_chunk_start = re.compile('\\.W')\n",
        "\n",
        "# process the query data\n",
        "cran_qry_data = defaultdict(dict)\n",
        "\n",
        "for n in range(0,len(cran_qry_list)):\n",
        "  line = cran_qry_list[n]\n",
        "  _ , question = re.split(qry_chunk_start,line)\n",
        "  cran_qry_data[n+1]['question'] = question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvoc52JyH67t"
      },
      "source": [
        "Relevance assesments are saved as numpy and parsed to dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A03hCq3-9L6F"
      },
      "source": [
        "# process relevance assesments with rating\n",
        "\n",
        "cran_rel_data = open(PATH_TO_CRAN_REL)\n",
        "cran_np = np.loadtxt(cran_rel_data, dtype=int)\n",
        "\n",
        "cran_rel_rat = defaultdict(list)\n",
        "for row in cran_np:\n",
        "  cran_rel_rat[row[0]].append(tuple(row[1:])) \n",
        "\n",
        "# process relevance assesments without rating\n",
        "cran_rel = defaultdict(list)\n",
        "\n",
        "with open (PATH_TO_CRAN_REL,'r') as f:\n",
        "  for line in f:\n",
        "    line = re.split(' ',line)\n",
        "    cran_rel[int(line[0])].append(line[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3y2zrw_GqBZ"
      },
      "source": [
        "![alternativer Text](https://)### Create index for LISA corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ewU0omCuD-"
      },
      "source": [
        "Create an index for the Cranfield corpus. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOTyG_hrzMi7"
      },
      "source": [
        "\n",
        "es.indices.create(\"cranfield-corpus\")\n",
        "\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsTRUQzXC4yW"
      },
      "source": [
        "Index all the documents that are processed to the created index in elasticsearch:\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/#example-usage))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmkqo-giYGue"
      },
      "source": [
        "cran_index = \"cranfield-corpus\"\n",
        "\n",
        "for ID, doc_data in cran_txt_data.items():\n",
        "  es.index(index=cran_index, id=ID, body=doc_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odi7MUA2DAJc"
      },
      "source": [
        "Verify if everthing went right by printing it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktDchP1Jl992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "99287afc-8fba-44f0-a10c-6746c31fb185"
      },
      "source": [
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yellow open security-auditlog-2020.07.26 3KQWvUwlRHOYj3AeOYcfkg 1 1   31 0  75.6kb  75.6kb\n",
            "yellow open security-auditlog-2020.07.27 C3A4S_lhSC-y5NEvhhhAAg 1 1   20 0  36.9kb  36.9kb\n",
            "yellow open pragmalingu-test             f_mKvLYwT9uqrczeykbXxw 1 1    0 0    208b    208b\n",
            "yellow open cranfield-corpus             _d6ZOeRPRGOnb2ftarkP3Q 1 1 1400 0   1.6mb   1.6mb\n",
            "green  open .kibana_92668751_admin_1     VkJe6jjiSMSj77PJCniIZQ 1 0    1 0   3.8kb   3.8kb\n",
            "yellow open security-auditlog-2020.07.28 sLtcImDYTmafKHDoxhmg8g 1 1   22 0  72.3kb  72.3kb\n",
            "green  open stemmer-cranfield-corpus     sNr-pythTYWzdQtzkPDuAw 1 0 1400 0   1.7mb   1.7mb\n",
            "yellow open security-auditlog-2020.07.29 s_wFvB3PTeKWfVSZFdlgOw 1 1    9 0 123.3kb 123.3kb\n",
            "green  open .kibana_92668751_admin_2     jqG35SnPSluE8FaWoIYZmg 1 0    1 0   3.8kb   3.8kb\n",
            "yellow open adi-corpus                   GcMzBe2bTC28dNE6-UsucQ 1 1   82 0 130.5kb 130.5kb\n",
            "green  open .kibana_1                    JQnZPkFUSmO1fr5WE0TSzw 1 0    0 0    208b    208b\n",
            "yellow open medline-corpus               coqclML2SG-zVrvvg2M1Sg 1 1 1032 0   1.1mb   1.1mb\n",
            "green  open .opendistro_security         aOOHbiN_QwKao4FpRXXu7w 1 0    7 1  45.4kb  45.4kb\n",
            "green  open .tasks                       Ak1djmqKRuqDf9Dw-TLwKQ 1 0    1 0   6.4kb   6.4kb\n",
            "green  open stemmer_index                pnka52e8RcWl197aO2MPjw 1 0    1 0  10.2kb  10.2kb\n",
            "green  open stemmer_index1               rrkXJIfIRgm-FXmC7Ftk7A 1 0    1 0   4.5kb   4.5kb\n",
            "green  open stemmer-index                jrQLMtPxQnum00eNjeJerQ 1 0    1 0   4.5kb   4.5kb\n",
            "yellow open cacm-corpus                  Mj4b0GjXRMi5vW4euWh4UA 1 1 3204 0   4.2mb   4.2mb\n",
            "yellow open security-auditlog-2020.07.24 RtdL1IsqTZu0wcDOgvGIgA 1 1   96 0 163.7kb 163.7kb\n",
            "yellow open cisi-corpus                  eZpgz3IpSge1coQHmsTb7A 1 1 1460 0   2.5mb   2.5mb\n",
            "yellow open security-auditlog-2020.07.25 3B4O7qOhQx2xPKMp7cYIiw 1 1    8 0 110.9kb 110.9kb\n",
            "yellow open stemmer-test-index           nvJ8HZbpSZKDf5P5SNFmfg 1 1    1 0   4.7kb   4.7kb\n",
            "yellow open security-auditlog-2020.07.30 NnDU9kvORb2yutAOIeXr-g 1 1   10 0 141.6kb 141.6kb\n",
            "yellow open security-auditlog-2020.07.31 M3E0K2AzQVebCtny8uczyg 1 1   25 0  99.6kb  99.6kb\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsHupiWaGwNe"
      },
      "source": [
        "### Use Corpus in Ranking API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lcDJsQ6DYP6"
      },
      "source": [
        "Use the ranking evaluation API from elasticsearch to evaluate the corpus:\n",
        "\n",
        "(For more information see the [python documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html?highlight=_rank_eval#elasticsearch.Elasticsearch.rank_eval) and the [Elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html#search-rank-eval))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pMooLiooPqA"
      },
      "source": [
        "cran_index = \"cranfield-corpus\"\n",
        "\n",
        "def create_query_body(query_dict, rel_dict, index_name):\n",
        "  \"\"\"\n",
        "  The function creates a request for every query in query_dict and rates the relevant documents with rel_dict to 1.\n",
        "  The index name has to be the same as from the documents your looking at.\n",
        "  An evaluation body for the elasticsearch ranking API is returned.\n",
        "  \"\"\"\n",
        "  eval_body = {\n",
        "      \"requests\":'',\n",
        "      \"metric\": {\n",
        "          \"recall\": {\n",
        "              \"relevant_rating_threshold\": 1,\n",
        "              \"k\": 20\n",
        "              }\n",
        "      }\n",
        "  }\n",
        "  requests = [] \n",
        "  current_request = defaultdict(lambda: defaultdict())\n",
        "  current_rel = {\"_index\": index_name, \"_id\": '', \"rating\": int}\n",
        "  for query_ID, query_txt in query_dict.items():\n",
        "    current_query = {\"query\": { \"multi_match\": { \"query\": '' , \"fields\" : [\"title\",\"text\"]}}}\n",
        "    current_query[\"query\"][\"multi_match\"][\"query\"] = query_txt['question']\n",
        "    current_request[\"id\"] = 'Query_'+str(query_ID)\n",
        "    current_request[\"request\"] = current_query.copy()\n",
        "    current_request[\"ratings\"] = [{\"_index\": index_name, \"_id\": str(el), \"rating\": 1} for el in rel_dict[query_ID]]\n",
        "    requests.append(current_request.copy())\n",
        "  eval_body[\"requests\"] = requests\n",
        "  return eval_body\n",
        "\n",
        "cran_create = create_query_body(cran_qry_data, cran_rel, cran_index)\n",
        "\n",
        "cran_eval_body = json.dumps(cran_create)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlp_n2deHBhb"
      },
      "source": [
        "Print results of Ranking API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ECC6vgxHA77"
      },
      "source": [
        "cran_res = es.rank_eval(cran_eval_body, cran_index)\n",
        "\n",
        "print(json.dumps(cran_res, indent=4, sort_keys=True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}