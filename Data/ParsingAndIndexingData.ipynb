{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParsingAndIndexingData.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOOFtd9W8TR1Go6hm0/7O3C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragmalingu/experiments/blob/master/Data/ParsingAndIndexingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Un-oDrHnDui"
      },
      "source": [
        "# Setup an Elasticsearch Instance in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQiAH-sinDum"
      },
      "source": [
        "Everthing to connect to Elasticsearch, for detailed explaination see [this Notebook.](https://)\n",
        "Download:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdccmnTUnDup"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "# download elasticsearch\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.1-linux-x86_64.tar.gz -q\n",
        "!tar -xzf elasticsearch-7.9.1-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.9.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxgS1p4_-zun"
      },
      "source": [
        "Start a local server:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8rAj0Ti-pOC",
        "outputId": "735c2877-abad-491d-af6b-f33f9f087841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# start server\n",
        "es_server = Popen(['elasticsearch-7.9.1/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                 )\n",
        "# client-side\n",
        "!pip install elasticsearch -q\n",
        "from elasticsearch import Elasticsearch\n",
        "from datetime import datetime\n",
        "es = Elasticsearch([\"localhost:9200/\"])\n",
        "#wait a bit\n",
        "import time\n",
        "time.sleep(30)\n",
        "es.ping()  # got True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▌                              | 10kB 3.6MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 51kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 71kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 81kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 92kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 102kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 112kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 122kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 133kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 143kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 153kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 163kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 174kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 184kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 194kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 204kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 215kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 2.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylsP46LvYXxp"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrwEamOKAJKI"
      },
      "source": [
        "Get different corpora, format them and feed them to elasticsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjiZiQqBB1ax"
      },
      "source": [
        "## ADI Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCLMzf84D589"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugn-mmHEJk0T"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/adi/).  <br>\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/private_experiments/blob/adi_corpus/ADICorpus.ipynb) or for parsing in generel read [this guide](https://)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-hYomDBB1ay"
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/adi/adi.tar.gz\n",
        "!tar -xf adi.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variables\n",
        "PATH_TO_ADI_TXT = '/content/ADI.ALL'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text and query files\n",
        "\n",
        "ID_marker = re.compile('\\.I')\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read().replace('\\n',\" \")\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "adi_txt_list = get_data(PATH_TO_ADI_TXT, ID_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "adi_title_start = re.compile('\\.T')\n",
        "adi_author_start = re.compile('\\.A')\n",
        "adi_text_start = re.compile('\\.W')\n",
        "\n",
        "adi_txt_data = defaultdict(dict)\n",
        "\n",
        "for line in adi_txt_list:\n",
        "  entries = re.split(adi_title_start,line,1)\n",
        "  id = entries[0].strip()\n",
        "  no_id = entries[1]\n",
        "  if len(re.split(adi_author_start, no_id,1)) > 1:\n",
        "    no_id_entries = re.split(adi_author_start, no_id,1)\n",
        "    adi_txt_data[id]['title'] = no_id_entries[0]\n",
        "    no_title = no_id_entries[1]\n",
        "    no_title_entries = re.split(adi_text_start, no_title)\n",
        "    adi_txt_data[id]['author'] = no_title_entries[0]\n",
        "    adi_txt_data[id]['text'] = no_title_entries[1]\n",
        "  else:\n",
        "    no_id_entries = re.split(adi_text_start, no_id)\n",
        "    adi_txt_data[id]['title'] = no_id_entries[0]\n",
        "    adi_txt_data[id]['text'] = no_id_entries[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn4N-oo-E8dK"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAeb_ZObE8dN"
      },
      "source": [
        "Create an index for the adi corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUqtBwO3E8dQ"
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "adi_index = \"adi-corpus\"\n",
        "es.indices.create(adi_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in adi_txt_data.items():\n",
        "  es.index(index=adi_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xRvatFztoo"
      },
      "source": [
        "## Cranfield Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3ltU26xDvm2"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fS6lVgXAO9F"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).  <br>\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/private_experiments/blob/cranfield_corpus/CranfieldCorpus.ipynb) or for parsing in generel read [this guide](https://)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsE0BqpwjrI1",
        "outputId": "1640201e-2064-43c4-af05-f4fd0650e0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
        "!tar -xf cran.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variables\n",
        "PATH_TO_CRAN_TXT = '/content/cran.all.1400'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text entries from the text and query files\n",
        "\n",
        "ID_marker = re.compile('\\.I')\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read().replace('\\n',\" \")\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "cran_txt_list = get_data(PATH_TO_CRAN_TXT, ID_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "cran_chunk_start = re.compile('\\.[A,B,T,W]')\n",
        "cran_txt_data = defaultdict(dict)\n",
        "\n",
        "for line in cran_txt_list:\n",
        "  entries= re.split(cran_chunk_start,line)\n",
        "  id = entries[0].strip()\n",
        "  title = entries[1]\n",
        "  author = entries[2]\n",
        "  publication_date = entries[3]\n",
        "  text = entries[4]\n",
        "  cran_txt_data[id]['title'] = title\n",
        "  cran_txt_data[id]['author'] = author\n",
        "  cran_txt_data[id]['publication_date'] = publication_date\n",
        "  cran_txt_data[id]['text'] = text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-16 06:31:35--  http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
            "Resolving ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)... 130.209.240.253\n",
            "Connecting to ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)|130.209.240.253|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 506960 (495K) [application/gzip]\n",
            "Saving to: ‘cran.tar.gz’\n",
            "\n",
            "cran.tar.gz         100%[===================>] 495.08K  1004KB/s    in 0.5s    \n",
            "\n",
            "2020-09-16 06:31:36 (1004 KB/s) - ‘cran.tar.gz’ saved [506960/506960]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n11sJxdEOqk"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dex8ehBEXAz"
      },
      "source": [
        "Create an index for the Cranfield corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs-sHqHZEXA1"
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "cran_index = \"cranfield-corpus\"\n",
        "es.indices.create(cran_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in cran_txt_data.items():\n",
        "  es.index(index=cran_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0ZqWC9JuKbe"
      },
      "source": [
        "## CACM Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIO4lnyXD7dW"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDc4PrF_uKbi"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cacm/).  <br>\n",
        "For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLroBvrduKbn"
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cacm/cacm.tar.gz\n",
        "!tar -xf cacm.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "\n",
        "PATH_TO_CACM_TXT = '/content/cacm.all'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text and query files\n",
        "\n",
        "ID_marker = re.compile('^\\.I',re.MULTILINE)\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read()\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "cacm_txt_list = get_data(PATH_TO_CACM_TXT, ID_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "cacm_chunk_title = re.compile('\\.[T]\\n')\n",
        "cacm_chunk_txt = re.compile('\\n\\.W\\n') # not enough\n",
        "cacm_chunk_txt_pub = re.compile('\\.[W,B]')\n",
        "cacm_chunk_publication = re.compile('\\.[B]\\n')\n",
        "cacm_chunk_author = re.compile('^\\.[A]\\n', re.MULTILINE)\n",
        "cacm_chunk_author_add_cross = re.compile('^\\.[A,N,X]\\n',re.MULTILINE) # not enough\n",
        "cacm_chunk_add_cross = re.compile('\\.[B,N,X]\\n')\n",
        "\n",
        "\n",
        "cacm_txt_data = defaultdict(dict)\n",
        "\n",
        "for line in cacm_txt_list:\n",
        "  entries= re.split(cacm_chunk_title,line)\n",
        "  id = entries[0].strip() #save id\n",
        "  no_id = entries[1]\n",
        "\n",
        "  if len(re.split(cacm_chunk_txt, no_id)) == 2: # is there text\n",
        "    no_id_entries = re.split(cacm_chunk_txt_pub, no_id,1)\n",
        "    cacm_txt_data[id]['title'] = no_id_entries[0].strip() # save title\n",
        "    cacm_txt_data[id]['text'] = no_id_entries[1].strip() # save text\n",
        "    no_title_txt = no_id_entries[1]\n",
        "\n",
        "    if len(re.split(cacm_chunk_author, no_title_txt)) == 2: # is there a auhtor\n",
        "      no_title_entries = re.split(cacm_chunk_author_add_cross, no_title_txt)\n",
        "      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date\n",
        "      cacm_txt_data[id]['author'] = no_title_entries[1].strip() # save athor\n",
        "      cacm_txt_data[id]['add_date'] = no_title_entries[2].strip() # save add date\n",
        "      cacm_txt_data[id]['cross-references'] = no_title_entries[3].strip() # save cross-references\n",
        "\n",
        "    else:\n",
        "      no_title_entries = re.split(cacm_chunk_publication, no_title_txt)\n",
        "      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date\n",
        "      cacm_txt_data[id]['add_date'] = no_title_entries[1].strip() # save add date\n",
        "      cacm_txt_data[id]['cross-references'] = no_title_entries[1].strip() # save cross-references\n",
        "\n",
        "  else:\n",
        "    no_id_entries = re.split(cacm_chunk_publication, no_id,1)\n",
        "    cacm_txt_data[id]['title'] = no_id_entries[0].strip() # save title\n",
        "    no_title = no_id_entries[1]\n",
        "\n",
        "    if len(re.split(cacm_chunk_author, no_title,1)) == 2: # is there a auhtor\n",
        "      no_title_entries = re.split(cacm_chunk_author_add_cross, no_title)\n",
        "      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date\n",
        "      cacm_txt_data[id]['author'] = no_title_entries[1].strip() # save athor\n",
        "      cacm_txt_data[id]['add_date'] = no_title_entries[2].strip() # save add date\n",
        "      cacm_txt_data[id]['cross-references'] = no_title_entries[3].strip() # save cross-references\n",
        "\n",
        "    else:\n",
        "      no_title_entries = re.split(cacm_chunk_add_cross, no_title)\n",
        "      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date\n",
        "      cacm_txt_data[id]['add_date'] = no_title_entries[1].strip() # save add date\n",
        "      cacm_txt_data[id]['cross-references'] = no_title_entries[2].strip() # save cross-references"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTgu3CVzEJ2u"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k25W2uvyGwn9"
      },
      "source": [
        "Create an index for the CACM corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaRQfFQdGymV"
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "cacm_index = \"cacm-corpus\"\n",
        "es.indices.create(cacm_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in cacm_txt_data.items():\n",
        "  es.index(index=cacm_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h46hrLuPNPvc"
      },
      "source": [
        "## CISI Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FtXXPStD6vI"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhEMgc2JNPve"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/).  <br>\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/private_experiments/blob/cisi_corpus/CISICorpus.ipynb) or for parsing in generel read [this guide](https://)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdn3IdFvNPvi"
      },
      "source": [
        "\n",
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
        "!tar -xf cisi.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "PATH_TO_CISI_TXT = '/content/CISI.ALL'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text file\n",
        "\n",
        "ID_marker = re.compile('^\\.I',re.MULTILINE)\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read()\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "cisi_txt_list = get_data(PATH_TO_CISI_TXT, ID_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "cisi_title_start = re.compile('[\\n]\\.T')\n",
        "cisi_author_start = re.compile('[\\n]\\.A')\n",
        "cisi_date_start = re.compile('[\\n]\\.B')\n",
        "cisi_text_start = re.compile('[\\n]\\.W')\n",
        "cisi_cross_start = re.compile('[\\n]\\.X')\n",
        "\n",
        "cisi_txt_data = defaultdict(dict)\n",
        "\n",
        "for line in cisi_txt_list:\n",
        "  entries = re.split(cisi_title_start,line,1)\n",
        "  id = entries[0].strip()#save the id\n",
        "  no_id = entries[1] \n",
        "  \n",
        "  if len(re.split(cisi_author_start, no_id)) >= 2: # is there just one author?\n",
        "    no_id_entries = re.split(cisi_author_start, no_id,1)\n",
        "    cisi_txt_data[id]['title'] = no_id_entries[0].strip() # save title\n",
        "    no_title = no_id_entries[1]\n",
        "\n",
        "    if len(re.split(cisi_date_start, no_title)) > 1: # is there a publication date?\n",
        "      no_title_entries = re.split(cisi_date_start, no_title)\n",
        "      cisi_txt_data[id]['author'] = no_title_entries[0].strip() # save athour\n",
        "      no_author = no_title_entries[1]\n",
        "      no_author_entries = re.split(cisi_text_start, no_author)\n",
        "      cisi_txt_data[id]['publication_date'] = no_author_entries[0].strip() # save publication date\n",
        "      no_author_date = no_author_entries[1]\n",
        "    else:\n",
        "      no_title_entries = re.split(cisi_text_start, no_title)\n",
        "      cisi_txt_data[id]['author'] = no_title_entries[0].strip() # save athour\n",
        "      no_author_date = no_title_entries[1]\n",
        "\n",
        "  else:\n",
        "    no_id_entries = re.split(cisi_author_start, no_id)\n",
        "    cisi_txt_data[id]['title'] = no_id_entries[0].strip() # save title\n",
        "    cisi_txt_data[id]['author'] = no_id_entries[1].strip() # save first author\n",
        "    no_title_entries = re.split(cisi_text_start, no_title)\n",
        "    cisi_txt_data[id]['author'] += ','+no_title_entries[0].strip() # save second athour\n",
        "    no_author_date = no_title_entries[1]\n",
        "\n",
        "  last_entries = re.split(cisi_cross_start, no_author_date)\n",
        "  cisi_txt_data[id]['text'] = last_entries[0].strip() # save text\n",
        "  cisi_txt_data[id]['cross-refrences'] = last_entries[1].strip() # save cross refrences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5SN1aovELJr"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVSqjrHmGnnt"
      },
      "source": [
        "Create an index for the CISI corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slNB44g9GqdQ"
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "cisi_index = \"cisi-corpus\"\n",
        "es.indices.create(cisi_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in cisi_txt_data.items():\n",
        "  es.index(index=cisi_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNYbwKJ5IWa_"
      },
      "source": [
        "## LISA Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fg2D5bCD8NF"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313f1qVFIWbH"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/lisa/).  <br>\n",
        "For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6NEluV1IWbP",
        "outputId": "58f8f5ec-ac5c-423e-bd78-6afda2e8988e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/lisa/lisa.tar.gz\n",
        "!tar -xf lisa.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "\n",
        "PATH_TO_LISA_TXT = '/content/'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# get the text files\n",
        "\n",
        "file_regex = re.compile('LISA[0-5]')\n",
        "lisa_files = [i for i in os.listdir(PATH_TO_LISA_TXT) if os.path.isfile(os.path.join(PATH_TO_LISA_TXT,i)) and re.match(file_regex,i)]\n",
        "\n",
        "txt_entry_marker = re.compile('\\*{44}',re.MULTILINE)\n",
        "\n",
        "def get_data(PATH_TO_FILES, marker):\n",
        "  \"\"\"\n",
        "  Reads multiple files and spilts text into entries at the entry marker.\n",
        "  The 'marker' contains the regex at which we want to split\n",
        "  Pops last element since it's empty.\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILES,'r') as f:\n",
        "    text = f.read().replace('     ','')\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop()\n",
        "  return lines\n",
        "\n",
        "lisa_txt_list = []\n",
        "for name in lisa_files: \n",
        "  lisa_txt_list.extend(get_data(PATH_TO_LISA_TXT+name, txt_entry_marker))\n",
        "\n",
        "# process text file\n",
        "\n",
        "doc_strip = re.compile('\\n?Document {1,2}')\n",
        "\n",
        "lisa_txt_list_stripped = []\n",
        "lisa_txt_data = defaultdict(dict)\n",
        "\n",
        "for el in lisa_txt_list:\n",
        "  lisa_txt_list_stripped.append(re.sub(doc_strip,'', el))\n",
        "\n",
        "for entry in lisa_txt_list_stripped:\n",
        "  parts = entry.split('\\n')\n",
        "  empty_index = parts.index('')\n",
        "  ID = parts[0].strip(' ')\n",
        "  title = parts[1:empty_index]\n",
        "  text = parts[empty_index+1:]\n",
        "  lisa_txt_data[ID]['title'] = ''.join(title)\n",
        "  lisa_txt_data[ID]['text'] = ''.join(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-06 08:42:24--  http://ir.dcs.gla.ac.uk/resources/test_collections/lisa/lisa.tar.gz\n",
            "Resolving ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)... 130.209.240.253\n",
            "Connecting to ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)|130.209.240.253|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1227725 (1.2M) [application/gzip]\n",
            "Saving to: ‘lisa.tar.gz’\n",
            "\n",
            "lisa.tar.gz         100%[===================>]   1.17M  1.53MB/s    in 0.8s    \n",
            "\n",
            "2020-10-06 08:42:25 (1.53 MB/s) - ‘lisa.tar.gz’ saved [1227725/1227725]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANLR0RmkEIb1"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuX-GdTaHErP"
      },
      "source": [
        "Create an index for the Lisa corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knnX1B-RHGvT",
        "outputId": "c38b52cf-7d02-4dac-c69e-34171c11af76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "lisa_index = \"lisa-corpus\"\n",
        "es.indices.create(lisa_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in lisa_txt_data.items():\n",
        "  es.index(index=lisa_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "green  open hunspell-cisi-corpus                  EnuNB6nqRBe8F50JqJkLrA 1 0  1460    0   2.6mb   2.6mb\n",
            "yellow open security-auditlog-2020.07.26          3KQWvUwlRHOYj3AeOYcfkg 1 1    31    0  75.6kb  75.6kb\n",
            "yellow open security-auditlog-2020.07.27          C3A4S_lhSC-y5NEvhhhAAg 1 1    20    0  36.9kb  36.9kb\n",
            "yellow open security-auditlog-2020.07.28          sLtcImDYTmafKHDoxhmg8g 1 1    22    0  72.3kb  72.3kb\n",
            "yellow open bert-text-title-cacm-corpus           yjXirDDnRMatjQAG6dKfvA 1 1  1586    0  57.7mb  57.7mb\n",
            "yellow open security-auditlog-2020.07.29          s_wFvB3PTeKWfVSZFdlgOw 1 1     9    0 123.3kb 123.3kb\n",
            "green  open snowball-medline-corpus               OpgXBcIrRnma8hWRj3YeSg 1 0  1032    0     1mb     1mb\n",
            "yellow open bert-text-adi-corpus                  3IGPlx4iRaODZ2R8vNfKzA 1 1    82    0   1.4mb   1.4mb\n",
            "green  open stemmer-cacm-corpus                   xDyo3niKQ2GPnqOwqeB1YQ 1 0  3204    0     4mb     4mb\n",
            "green  open .opendistro_security                  aOOHbiN_QwKao4FpRXXu7w 1 0     7    1  81.7kb  81.7kb\n",
            "green  open pragmalingu-stemming-lisa-corpus      LM2ejlB2RVyK4BtP7lVkVw 1 0  6000    0   9.6mb   9.6mb\n",
            "yellow open bert-text-cranfield-corpus            iPUmAkJhROalScqCKV4ahg 1 1  1400 2820  79.1mb  79.1mb\n",
            "green  open stemming-index                        ZhsePaZSSRyaUJybb5ejQg 1 0     1    0   4.5kb   4.5kb\n",
            "yellow open cisi-corpus                           TYV93oPETnSojbHlxP_tUw 1 1  1460    0   2.6mb   2.6mb\n",
            "yellow open security-auditlog-2020.07.24          RtdL1IsqTZu0wcDOgvGIgA 1 1    96    0 163.7kb 163.7kb\n",
            "yellow open security-auditlog-2020.07.25          3B4O7qOhQx2xPKMp7cYIiw 1 1     8    0 110.9kb 110.9kb\n",
            "green  open stemming-lisa-corpus                  bXamS6xBTsGfMptb5gZfdg 1 0  6000    0   9.7mb   9.7mb\n",
            "green  open snowball-cacm-corpus                  -hg_0d21SfWJI9xkNGB5Bg 1 0  3204    0     4mb     4mb\n",
            "yellow open security-auditlog-2020.08.09          lrjxjXmYRKGwLNOVZVxu9Q 1 1     8    0   109kb   109kb\n",
            "yellow open security-auditlog-2020.08.06          JZ1UQBo7TEG4u2Z7W2eI0g 1 1     7    0  95.4kb  95.4kb\n",
            "yellow open security-auditlog-2020.08.05          TD8u8865TJSS_WesJW9y-Q 1 1    12    0  56.1kb  56.1kb\n",
            "green  open stemmer-adi-corpus                    J1ucnpB9Se-0oGCOnUgK8A 1 0    82    0  80.9kb  80.9kb\n",
            "yellow open security-auditlog-2020.08.08          DB4gwc5LQpqaU7CnyvfhAg 1 1     6    0    88kb    88kb\n",
            "green  open .kibana_92668751_admin_1              VkJe6jjiSMSj77PJCniIZQ 1 0     1    0   3.8kb   3.8kb\n",
            "yellow open security-auditlog-2020.08.07          WHVdg4eiSNiKyF8vGuPqOA 1 1    13    0  66.4kb  66.4kb\n",
            "green  open stemmer-cranfield-corpus              vqH6rkxkRUOFY9v3BSr2wQ 1 0  1400    0   1.5mb   1.5mb\n",
            "green  open pragmalingu-hunspell-adi-corpus       DlmYZUa-QoWOicNiS_tfmA 1 0    82    0 130.5kb 130.5kb\n",
            "green  open .kibana_92668751_admin_2              jqG35SnPSluE8FaWoIYZmg 1 0     1    0   3.8kb   3.8kb\n",
            "yellow open lisa-corpus                           qV43RXqbTfiPiuDR232KOA 1 1   271    0   2.9mb   2.9mb\n",
            "green  open pragmalingu-stemming-adi-corpus       wRdAQX-HRaSErPxt27ROmg 1 0    82    0  96.4kb  96.4kb\n",
            "green  open .tasks                                Ak1djmqKRuqDf9Dw-TLwKQ 1 0     1    0   6.4kb   6.4kb\n",
            "green  open hunspell-lisa-corpus                  RysLgUYhRXmMxRRnnFftug 1 0  6000    0   9.8mb   9.8mb\n",
            "yellow open bert-text-title-medline-corpus        hI6BJM6CQm-uFpIQmIXvZQ 1 1  1033    0  19.4mb  19.4mb\n",
            "yellow open npl-corpus                            K0KrLtYlSiKjQTLne8zwCg 1 1 11429    0   4.6mb   4.6mb\n",
            "green  open dictionary-stemming-index             JEjh7rLrQRC7xzasocnAYw 1 0     1    0   4.5kb   4.5kb\n",
            "yellow open security-auditlog-2020.08.02          hStsCYpbS-uhuSd3dyBFvg 1 1    15    0  95.5kb  95.5kb\n",
            "yellow open security-auditlog-2020.08.01          WO6XBXRnSPmu-Q3IIzXWdg 1 1    14    0  82.2kb  82.2kb\n",
            "green  open kstem-cisi-corpus                     YJlug182Q42NhlfWBSCqNQ 1 0  1460    0   2.4mb   2.4mb\n",
            "yellow open security-auditlog-2020.08.04          1h2xSbNVQ9-kZx9TAzjOEA 1 1     8    0 116.9kb 116.9kb\n",
            "yellow open bert-text-cisi-corpus                 ndZe5Tu9SFC5dWGqvGvuKw 1 1  1460    0    27mb    27mb\n",
            "yellow open security-auditlog-2020.08.03          Q2tbS9UUS3y5NIBxLJDHoA 1 1    26    0 123.1kb 123.1kb\n",
            "yellow open pragmalingu-cacm-corpus               PHpk31BeSKyeFWm4Eon56g 1 1  3204    0   4.1mb   4.1mb\n",
            "yellow open security-auditlog-2020.07.30          NnDU9kvORb2yutAOIeXr-g 1 1    10    0 141.6kb 141.6kb\n",
            "yellow open security-auditlog-2020.07.31          M3E0K2AzQVebCtny8uczyg 1 1    27    0   132kb   132kb\n",
            "green  open hunspell-time-corpus                  WDIR8wdWSfSE06qrm3zHCg 1 0   423    0   1.8mb   1.8mb\n",
            "yellow open security-auditlog-2020.09.29          Ya_BHQgDTzmjqcsjzDNUqQ 1 1    21    0  53.1kb  53.1kb\n",
            "yellow open security-auditlog-2020.09.28          gSjL9y5uT62Qcuzavle_9A 1 1    28    0 150.5kb 150.5kb\n",
            "yellow open cranfield-corpus                      -NFVyQ3nTYeJCa_Tn90Yuw 1 1  1400    0   1.7mb   1.7mb\n",
            "green  open porter-cisi-corpus                    zYDAX1uTSUOL27GYEE063g 1 0  1460    0   2.4mb   2.4mb\n",
            "green  open pragmalingu-stemming-cranfield-corpus 37triTgPQ5W2E8QV9xJgeA 1 0  1400    0   1.6mb   1.6mb\n",
            "yellow open bert-text-title-adi-corpus            NE9FV2zbSiWHTx-TjwEyLQ 1 1    82    0   2.9mb   2.9mb\n",
            "green  open stemming-cacm-corpus                  WW1gvxHeTk66Sj_Tp54Rpw 1 0  3204    0   4.1mb   4.1mb\n",
            "green  open hunspell-cranfield-corpus             MN6C1og0SjuKr8BrR6wEtA 1 0  1400    0   1.6mb   1.6mb\n",
            "green  open stemming-cisi-corpus                  CgNvnWf2RKuZQd2htUS0tA 1 0  1460    0   2.5mb   2.5mb\n",
            "yellow open security-auditlog-2020.09.25          aKEon0FYSCW7eMvPcoGAsw 1 1    29    0  44.2kb  44.2kb\n",
            "yellow open security-auditlog-2020.09.24          W0fkQU3bSp2tFJ8dil9EIA 1 1    14    0  77.2kb  77.2kb\n",
            "green  open hunspell-npl-corpus                   6a_WLqmeScy_a9L78vB8Lw 1 0 11429    0   4.6mb   4.6mb\n",
            "yellow open security-auditlog-2020.09.27          SM_WUAxMTnGrzaiA0IwUgg 1 1     6    0  91.2kb  91.2kb\n",
            "yellow open security-auditlog-2020.09.26          KlJuxYDOTmW8FAsGiiu3ag 1 1    17    0   126kb   126kb\n",
            "yellow open security-auditlog-2020.09.21          hRTfJYjSSBuYUrfKBVawoA 1 1    20    0  44.3kb  44.3kb\n",
            "yellow open security-auditlog-2020.09.20          vF1tpeLhRpSPbk8zGoqK6A 1 1    14    0  70.2kb  70.2kb\n",
            "yellow open security-auditlog-2020.09.23          tZhTrechT5SPpZDA4xMSCw 1 1    10    0 140.4kb 140.4kb\n",
            "yellow open stemming-medline-corpus               ld3Jn52eQrW7vQ4XL9o3jA 1 1  1032  323   1.4mb   1.4mb\n",
            "yellow open security-auditlog-2020.09.22          5nnKRUjhSMerGBOD3-3Xjg 1 1    15    0  88.1kb  88.1kb\n",
            "yellow open pragmalingu-cran-corpus               bDfOpaevTOmbENoJLNkcbw 1 1     0    0    208b    208b\n",
            "green  open pragmalingu-stemming-cisi-corpus      WwPu5fqcRmaLQQd_z_bHiQ 1 0  1460    0   2.4mb   2.4mb\n",
            "green  open hunspell-adi-corpus                   lhmnZdwMQ0W2ciAhGp3OtQ 1 0    82    0  77.8kb  77.8kb\n",
            "green  open kstem-cranfield-corpus                R0rE_NG6QG-2v2TkLlVu2g 1 0  1400    0   1.6mb   1.6mb\n",
            "yellow open bert-title-adi-corpus                 r019cxBdS1GSkfjTXILUdg 1 1    82    0   1.4mb   1.4mb\n",
            "green  open porter-adi-corpus                     w3SmWuERTSSfCreh6TQmpQ 1 0    82    0  80.9kb  80.9kb\n",
            "yellow open bert-toy_index                        sW9f4IBORtuNWC-GwR6zFA 1 1     0    0    208b    208b\n",
            "green  open pragmalingu-hunspell-cacm-corpus      oFdWaUsTRbOkhitdoJ5yNQ 1 0  3204    0   4.1mb   4.1mb\n",
            "green  open porter-cranfield-corpus               G0onoY2sTJeXr1-s0omTiA 1 0  1400    0   1.5mb   1.5mb\n",
            "yellow open bert-text-lisa-corpus                 h2AFmQ3nTpeHW3jgwZgMMw 1 1  6000    0 109.1mb 109.1mb\n",
            "yellow open adi-corpus                            ilRyd4TWSW6zdBrSXoaPYQ 1 1    82    0  93.8kb  93.8kb\n",
            "yellow open medline-corpus                        NYo-Zrt5QRqyEGN2DdziTA 1 1  1032    0   1.1mb   1.1mb\n",
            "yellow open pragmalingu-testindex                 BXtxCiBjRt2KzFwL-s3rzg 1 1     0    0    208b    208b\n",
            "yellow open bert-title-cacm-corpus                KKni0IdnQemeyMUxz4BKwg 1 1  3204    0  56.5mb  56.5mb\n",
            "yellow open bert-text-title-npl-corpus            -gt_SoocR9m0yN8XqHamZA 1 1  1955    0  63.7mb  63.7mb\n",
            "yellow open security-auditlog-2020.09.30          jMPVJfe8S7Kqwtv7wCDGOg 1 1    12    0  58.4kb  58.4kb\n",
            "yellow open bert-text-title-cranfield-corpus      G0cq9SCzQaWC-hY3g4P_cQ 1 1  1400    0    27mb    27mb\n",
            "yellow open pragmalingu-medline-corpus            MuWEYXs7RSmccmnhQ7K1Ag 1 1  1032    0   1.1mb   1.1mb\n",
            "green  open pragmalingu-hunspell-time-corpus      fCYCBeGOQ6ulY5g_bbMuSw 1 0   423    0   1.8mb   1.8mb\n",
            "green  open porter-medline-corpus                 pCZngrxjSS2FwFgxbUqNnA 1 0  1032    0     1mb     1mb\n",
            "green  open snowball-cranfield-corpus             W6kIploWQJGNMm6xwGtJbg 1 0  1400    0   1.5mb   1.5mb\n",
            "green  open hunspell-cacm-corpus                  AGqGK8XPQdKE8UnD1nn0EA 1 0  3204    0   4.1mb   4.1mb\n",
            "yellow open security-auditlog-2020.09.07          CXR4KyRQSkKTdnZJoNjyCA 1 1    13    0  68.8kb  68.8kb\n",
            "yellow open security-auditlog-2020.09.06          G1VALh86SISD8hKbzSh1zg 1 1    14    0  73.6kb  73.6kb\n",
            "yellow open security-auditlog-2020.09.09          aZSE5DtYSoesm1wi3Vwblg 1 1    21    0  59.6kb  59.6kb\n",
            "yellow open security-auditlog-2020.09.08          tOxuCA7yTAO1FWijI6cIbw 1 1    22    0  75.1kb  75.1kb\n",
            "green  open pragmalingu-hunspell-npl-corpus       sfyXOm31TFy231RuKu4i-w 1 0 11429    0   4.6mb   4.6mb\n",
            "yellow open bert-title-cranfield-corpus           i33xmrnURhKmE3Si3pVy7g 1 1  1400    0  24.5mb  24.5mb\n",
            "yellow open pragmalingu-cisi-corpus               CH3y6fjrR-6dlstL1FwRAQ 1 1  1460    0   2.4mb   2.4mb\n",
            "green  open stemming-time-corpus                  RntweNy1Rl2zhkwcpEL4wg 1 0   423    0   1.8mb   1.8mb\n",
            "yellow open bert-text-medline-corpus              kpXSN0BTQqiY34XylD-OZg 1 1  1033    0  19.5mb  19.5mb\n",
            "green  open pragmalingu-hunspell-medline-corpus   07itng1uScGEmUc6Urt5Ww 1 0  1032    0   1.1mb   1.1mb\n",
            "green  open kstem-cacm-corpus                     d33yOiPPSUKPCwT7Ch4lcQ 1 0  3204    0     4mb     4mb\n",
            "yellow open security-auditlog-2020.09.03          bGmqEn1DQ_awze1cwj1mqw 1 1    18    0 140.4kb 140.4kb\n",
            "green  open stemming-adi-corpus                   kiENOlQiRJaAgxE9IW6FvQ 1 0    82    0   125kb   125kb\n",
            "yellow open security-auditlog-2020.09.02          7bHBdl78TwG3PI6m00pmBg 1 1    30    0    53kb    53kb\n",
            "yellow open bert-text-cacm-corpus                 YDT0am_HRo2PpoN_UqUOgg 1 1  1586    0  29.9mb  29.9mb\n",
            "yellow open bert-text-title-cisi-corpus           8qsXt1RtTFi7752nj4fzZg 1 1  1460    0  52.7mb  52.7mb\n",
            "yellow open security-auditlog-2020.09.05          WDX_i4c6QbajnJUv95YW3Q 1 1    15    0  88.5kb  88.5kb\n",
            "yellow open security-auditlog-2020.09.04          SuZUyaqwQ-qoi5Kw132kuQ 1 1    18    0   134kb   134kb\n",
            "green  open hunspell-medline-corpus               kaZfLEjGSh-7ppGAqp2_AA 1 0  1032    0     1mb     1mb\n",
            "yellow open security-auditlog-2020.08.31          BLl9Uzp7SYqcxXKcILayUA 1 1    11    0  50.4kb  50.4kb\n",
            "yellow open security-auditlog-2020.08.30          cBGF4nl-RzC29G7uaFjcJA 1 1    14    0  87.5kb  87.5kb\n",
            "yellow open security-auditlog-2020.09.01          TwkdDyFCQbSSW9KunQPPag 1 1    15    0  86.1kb  86.1kb\n",
            "green  open pragmalingu-stemming-npl-corpus       xAFR74IAQXCZ_RKneyHDxw 1 0 11429    0   4.5mb   4.5mb\n",
            "yellow open bert-text-npl-corpus                  NLS-jVDsQyyl1jaCG_3FXw 1 1 11429    0 205.2mb 205.2mb\n",
            "yellow open security-auditlog-2020.09.18          hn8Nfi6WROChoCfNBBYE1g 1 1    19    0 148.9kb 148.9kb\n",
            "yellow open security-auditlog-2020.09.17          v3-KzsslTk6zCudRQtf-nw 1 1    17    0 125.7kb 125.7kb\n",
            "yellow open security-auditlog-2020.10.06          1K1xFu_VTHeMC5AOlZNt1A 1 1    18    0 113.5kb 113.5kb\n",
            "yellow open pragmalingu-lisa-corpus               D8mxK5M9TJaDrcliPpD3AQ 1 1  6000    0   9.8mb   9.8mb\n",
            "yellow open security-auditlog-2020.10.05          -4lNVGi5Sju5VRm9Cs1evw 1 1    10    0 142.5kb 142.5kb\n",
            "yellow open security-auditlog-2020.10.04          lV5GEY76QcmbUPE-np-wgQ 1 1    16    0    99kb    99kb\n",
            "yellow open security-auditlog-2020.09.19          wxAHlxz8QTaTFnnzjnwCdA 1 1    13    0  59.2kb  59.2kb\n",
            "yellow open security-auditlog-2020.10.03          sChQNesHRDGZvaHmEtsNqg 1 1     7    0  97.2kb  97.2kb\n",
            "yellow open security-auditlog-2020.10.02          DPXG_aESQqm7IWhSH9VGpw 1 1    19    0 133.8kb 133.8kb\n",
            "yellow open security-auditlog-2020.10.01          7f9C-03UTb6nRQTQ-NOD0w 1 1    10    0 136.6kb 136.6kb\n",
            "yellow open pragmalingu-med-corpus                _cylNq05TXunMH3MJT7Fxg 1 1     0    0    208b    208b\n",
            "green  open pragmalingu-stemming-medline-corpus   EFa3tglKQ12cHk48j6eU3w 1 0  1032    0     1mb     1mb\n",
            "yellow open pragmalingu-npl-corpus                U5nNwxEsRPqgSOXnPwVYpg 1 1 11429    0   4.5mb   4.5mb\n",
            "yellow open cacm-corpus                           rhbI9n0MQ52xRBTArIYJ1A 1 1  3204    0   4.2mb   4.2mb\n",
            "yellow open security-auditlog-2020.09.14          Fa9QmUapROeO02qdhJ4Fcw 1 1    15    0  90.4kb  90.4kb\n",
            "yellow open security-auditlog-2020.09.13          bB1BKk67TsCUCrSoGYVajA 1 1    14    0  74.4kb  74.4kb\n",
            "green  open stemmer-cisi-corpus                   BZmYMsnDRDe212bo9yXt7g 1 0  1460    0   2.4mb   2.4mb\n",
            "yellow open security-auditlog-2020.09.16          ehsCg5e3RqOEJNyGe4aKJA 1 1    23    0  93.8kb  93.8kb\n",
            "yellow open security-auditlog-2020.09.15          anhjTQw9R0yaUSictmhM2g 1 1    12    0  49.8kb  49.8kb\n",
            "green  open kstem-medline-corpus                  U-ApYghLTFuwBTLH-mAsCw 1 0  1032    0     1mb     1mb\n",
            "green  open snowball-adi-corpus                   YPWMUUKJTyqdUonKZPAFPg 1 0    82    0    88kb    88kb\n",
            "green  open kstem-adi-corpus                      M9-h9wyhRYygRzh0y-KkWQ 1 0    82    0  83.8kb  83.8kb\n",
            "yellow open security-auditlog-2020.09.10          xD88cIFpRGiWO6L5-LJgHw 1 1    23    0  88.5kb  88.5kb\n",
            "yellow open time-corpus                           Ge5qefcHRTuOn0USBWvZJA 1 1   423    0   1.9mb   1.9mb\n",
            "yellow open security-auditlog-2020.09.12          UYl5QhOVTNCo3oB628HOog 1 1    16    0 110.7kb 110.7kb\n",
            "green  open snowball-cisi-corpus                  m6dRp3LnT7KIgwcrVTLuJg 1 0  1460    0   2.4mb   2.4mb\n",
            "yellow open security-auditlog-2020.09.11          UAc_F3vdRW-FlBRgym-w6A 1 1    10    0 140.9kb 140.9kb\n",
            "green  open stemming-npl-corpus                   6pI9QC46QgigtrOkpHKBTw 1 0 11429    0   4.5mb   4.5mb\n",
            "yellow open pragmalingu-cranfield-corpus          yHSH7APhSS2sBYzRhoI9TA 1 1  1400  194   1.9mb   1.9mb\n",
            "yellow open bert-title-lisa-corpus                uUZ1QfqeQrijqJor-Djx5A 1 1     0    0    208b    208b\n",
            "yellow open security-auditlog-2020.08.17          kRp851-TSpu1iCFJql5wPA 1 1     1    0  14.3kb  14.3kb\n",
            "yellow open security-auditlog-2020.08.16          dCI-6KBiSci_ifoyDIn7_A 1 1     7    0 102.8kb 102.8kb\n",
            "yellow open pragmalingu-test                      f_mKvLYwT9uqrczeykbXxw 1 1     0    0    208b    208b\n",
            "yellow open security-auditlog-2020.08.19          ncA_JV_tR_i1TpTcODeZkA 1 1    20    0  44.9kb  44.9kb\n",
            "yellow open security-auditlog-2020.08.18          K6pELrmgQd6-pseO1uATlw 1 1    14    0  92.6kb  92.6kb\n",
            "yellow open pragmalingu-time-corpus               Ti3pMsBsQruxDuRa4VS3Ow 1 1   423    0   1.8mb   1.8mb\n",
            "green  open porter-cacm-corpus                    FFq5OFE7QUiLYhjzilmhiQ 1 0  3204    0     4mb     4mb\n",
            "yellow open bert-text-title-lisa-corpus           yvtahSemQj2iZM533HvJ8g 1 1     0    0    208b    208b\n",
            "yellow open pragmalingu-adi-corpus                a387FYRbSpiGJPkHoP1V0A 1 1    82    0 102.4kb 102.4kb\n",
            "green  open stemmer-medline-corpus                tHhj8zeiQESCi6WYHwiMsw 1 0  1032    0     1mb     1mb\n",
            "yellow open security-auditlog-2020.08.13          yZk0D0wkQmGRxmvCjipCjQ 1 1     7    0  90.2kb  90.2kb\n",
            "yellow open security-auditlog-2020.08.12          PdX1xjG4Q8ajOadxA2garA 1 1     9    0   118kb   118kb\n",
            "yellow open bert-text-time-corpus                 cd7tElZeR4eT1qU4KDvYhQ 1 1   423    0   9.5mb   9.5mb\n",
            "yellow open security-auditlog-2020.08.15          AtayPSVlTOeD04e2x8Juqg 1 1     7    0  94.8kb  94.8kb\n",
            "yellow open security-auditlog-2020.08.14          zdZfZ_IoTnCMihvvFrftWg 1 1    17    0   113kb   113kb\n",
            "green  open pragmalingu-hunspell-cisi-corpus      Lipw_cz_STOfgSziQVRdnQ 1 0  1460    0   2.5mb   2.5mb\n",
            "yellow open security-auditlog-2020.08.11          N9Z4QfE2R_qhl8GX9PC6tA 1 1     5    0  66.6kb  66.6kb\n",
            "yellow open security-auditlog-2020.08.10          fgRMsSClTUmhbUNoepe33g 1 1    16    0 118.2kb 118.2kb\n",
            "yellow open security-auditlog-2020.08.28          v8i6hCdbS5O7qBJZXqqKRg 1 1    28    0 173.3kb 173.3kb\n",
            "yellow open security-auditlog-2020.08.27          EOWYOzO4SmOalnha3d9hPw 1 1    14    0  84.4kb  84.4kb\n",
            "green  open stemming-cranfield-corpus             0ikc5xmcSXSk_cfSro9H3A 1 0  1400    0   1.6mb   1.6mb\n",
            "green  open pragmalingu-stemming-time-corpus      bPtwlNLmSxqRXzdrV_N92A 1 0   423    0   1.7mb   1.7mb\n",
            "yellow open security-auditlog-2020.08.29          NuqogCVMQOe_q9CluZ4tbw 1 1    10    0 141.7kb 141.7kb\n",
            "yellow open pragmalingu-bert-text-adi-corpus      snfMchkASSis09jh5t7NPA 1 1    82    0   1.5mb   1.5mb\n",
            "green  open pragmalingu-hunspell-lisa-corpus      YVofcAdYTAWgrNKaSwFENg 1 0  6000    0   9.8mb   9.8mb\n",
            "green  open .kibana_1                             JQnZPkFUSmO1fr5WE0TSzw 1 0     0    0    208b    208b\n",
            "green  open pragmalingu-stemming-cacm-corpus      ncHjX5dASgiVF-V2JazLZA 1 0  3204    0     4mb     4mb\n",
            "yellow open bert-title-cisi-corpus                flYeFZRiQxq8NZKrSxne9g 1 1  1460    0  25.7mb  25.7mb\n",
            "yellow open security-auditlog-2020.08.24          eAd4rXGWSI2pZRUWbgPxJg 1 1    12    0  49.2kb  49.2kb\n",
            "yellow open security-auditlog-2020.08.23          1Ze6xu98T6u73yLbXaRvvQ 1 1    12    0  52.9kb  52.9kb\n",
            "yellow open security-auditlog-2020.08.26          HO4_5BCaT-uRx3RaY6_Iww 1 1     7    0  92.7kb  92.7kb\n",
            "yellow open security-auditlog-2020.08.25          _lJ8RCrATU6p2I8B9QNMLQ 1 1    12    0  59.2kb  59.2kb\n",
            "yellow open security-auditlog-2020.08.20          NjmpWdKnTtWOJROC0gB1Vw 1 1    10    0 138.7kb 138.7kb\n",
            "yellow open security-auditlog-2020.08.22          BxWnzbMWQha_M2W4FFVCwA 1 1     9    0 130.9kb 130.9kb\n",
            "yellow open security-auditlog-2020.08.21          mVDOWXa8R1-u__x8H9PtFQ 1 1     8    0 111.9kb 111.9kb\n",
            "green  open pragmalingu-hunspell-cranfield-corpus x6P94HGDQ-GiMrtSPA6V8g 1 0  1400    0   1.6mb   1.6mb\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW6zgF8_JG4e"
      },
      "source": [
        "## Medline Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7qvVZxgDzzt"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp1yEVO8037d"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/med/).  <br>\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/private_experiments/blob/medline_corpus/MedlineCorpus.ipynb) or for parsing in generel read [this guide](https://)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yo-Qv89JG4q"
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/medl/med.tar.gz\n",
        "!tar -xf med.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variables\n",
        "PATH_TO_MED_TXT = '/content/MED.ALL'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text and query files\n",
        "\n",
        "ID_marker = re.compile('\\.I')\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read().replace('\\n',\" \")\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "med_txt_list = get_data(PATH_TO_MED_TXT, ID_marker)\n",
        "\n",
        "# process the text file\n",
        "\n",
        "chunk_start = re.compile('\\.W')\n",
        "med_txt_data = defaultdict(dict)\n",
        "\n",
        "def fill_dictionary(dictionary, chunk_list, marker, key_name):\n",
        "  for n in range(0,len(chunk_list)-1):\n",
        "    line = chunk_list[n+1]\n",
        "    _ , chunk = re.split(marker,line)\n",
        "    dictionary[n+1][key_name] = chunk.strip()\n",
        "\n",
        "fill_dictionary(med_txt_data, med_txt_list, chunk_start, 'text')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpvaXRK_ENmm"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_CEZbwuEs_W"
      },
      "source": [
        "Create an index for the Medline corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fksapREdEs_Y"
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "med_index = \"medline-corpus\"\n",
        "es.indices.create(med_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in med_txt_data.items():\n",
        "  es.index(index=med_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXbsI4B_34QH"
      },
      "source": [
        "## NPL Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W22w4GTsEBu3"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-XV8MnE34QK"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/npl/).  <br>\n",
        "For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAcUrS6w34QS"
      },
      "source": [
        "# download and unzip data\n",
        "\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/npl/npl.tar.gz\n",
        "!tar -xf npl.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "\n",
        "PATH_TO_NPL_TXT = '/content/doc-text'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# get the text, query and rel files\n",
        "\n",
        "txt_entry_marker = re.compile('\\n   /\\n')\n",
        "\n",
        "def get_data(PATH_TO_FILES, marker):\n",
        "  \"\"\"\n",
        "  Reads multiple files and spilts text into entries at the entry marker.\n",
        "  The 'marker' contains the regex at which we want to split\n",
        "  Pops last element since it's empty.\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILES,'r') as f:\n",
        "    text = f.read()\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop()\n",
        "  return lines\n",
        "\n",
        "npl_txt_list = get_data(PATH_TO_NPL_TXT, txt_entry_marker)\n",
        "\n",
        "# process the text file\n",
        "\n",
        "npl_txt_data = defaultdict(dict)\n",
        "\n",
        "for entry in npl_txt_list:\n",
        "  splitted = entry.split('\\n')\n",
        "  splitted = list(filter(None, splitted))\n",
        "  ID = splitted[0]\n",
        "  text = ' '.join(map(str, splitted[1:]))\n",
        "  npl_txt_data[ID]['text'] = text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNEqYeWxEC5W"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SP9glnXHQIl"
      },
      "source": [
        "Create an index for the NPL corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAYR7tGQHRqb"
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "npl_index = \"npl-corpus\"\n",
        "es.indices.create(npl_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in npl_txt_data.items():\n",
        "  es.index(index=npl_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzHvsKajyd8o"
      },
      "source": [
        "## Time Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsHyfhOHD8xd"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1OwnRY3yd8w"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/time/).  <br>\n",
        "For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHfj2Jzbyd83"
      },
      "source": [
        "# download and unzip data\n",
        "\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/time/time.tar.gz\n",
        "!tar -xf time.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "\n",
        "PATH_TO_TIME_TXT = '/content/TIME.ALL'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# get the text and query file\n",
        "\n",
        "txt_entry_marker = re.compile('\\*TEXT')\n",
        "\n",
        "def get_data(PATH_TO_FILES, marker):\n",
        "  \"\"\"\n",
        "  Reads multiple files and spilts text into entries at the entry marker.\n",
        "  The 'marker' contains the regex at which we want to split\n",
        "  Pops last element since it's empty.\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILES,'r') as f:\n",
        "    text = f.read()\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "time_txt_list = get_data(PATH_TO_TIME_TXT, txt_entry_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "page_split = re.compile('PAGE \\d{3}')\n",
        "\n",
        "time_txt_data = defaultdict(dict)\n",
        "ID = 1\n",
        "for entry in time_txt_list:\n",
        "  splitted = re.split(page_split,entry)\n",
        "  time_txt_data[ID]['text'] = splitted[1]\n",
        "  ID += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H93yMIMJEG3M"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ1f_D7WHKcp"
      },
      "source": [
        "Create an index for the Time corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPqxAtkkHL4x"
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "time_index = \"time-corpus\"\n",
        "es.indices.create(time_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in time_txt_data.items():\n",
        "  es.index(index=time_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}