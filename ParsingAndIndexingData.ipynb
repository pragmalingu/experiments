{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParsingAndIndexingData.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNH0Y8/mZYP/hkKeaTzCsp1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragmalingu/experiments/blob/master/ParsingAndIndexingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Un-oDrHnDui"
      },
      "source": [
        "# Setup an Elasticsearch Instance in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wQiAH-sinDum"
      },
      "source": [
        "Everthing to connect to Elasticsearch, for detailed explaination see [this Notebook.](https://)\n",
        "Download:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XdccmnTUnDup",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "# download elasticsearch\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.1-linux-x86_64.tar.gz -q\n",
        "!tar -xzf elasticsearch-7.9.1-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.9.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxgS1p4_-zun",
        "colab_type": "text"
      },
      "source": [
        "Start a local server:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8rAj0Ti-pOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "735c2877-abad-491d-af6b-f33f9f087841"
      },
      "source": [
        "# start server\n",
        "es_server = Popen(['elasticsearch-7.9.1/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                 )\n",
        "# client-side\n",
        "!pip install elasticsearch -q\n",
        "from elasticsearch import Elasticsearch\n",
        "from datetime import datetime\n",
        "es = Elasticsearch([\"localhost:9200/\"])\n",
        "#wait a bit\n",
        "import time\n",
        "time.sleep(30)\n",
        "es.ping()  # got True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▌                              | 10kB 3.6MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 51kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 71kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 81kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 92kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 102kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 112kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 122kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 133kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 143kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 153kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 163kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 174kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 184kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 194kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 204kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 215kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 2.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylsP46LvYXxp",
        "colab_type": "text"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrwEamOKAJKI",
        "colab_type": "text"
      },
      "source": [
        "Get different corpora, format them and feed them to elasticsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zjiZiQqBB1ax"
      },
      "source": [
        "## ADI Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCLMzf84D589",
        "colab_type": "text"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ugn-mmHEJk0T"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/adi/).  <br>\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/private_experiments/blob/adi_corpus/ADICorpus.ipynb) or for parsing in generel read [this guide](https://)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R-hYomDBB1ay",
        "colab": {}
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/adi/adi.tar.gz\n",
        "!tar -xf adi.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variables\n",
        "PATH_TO_ADI_TXT = '/content/ADI.ALL'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text and query files\n",
        "\n",
        "ID_marker = re.compile('\\.I')\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read().replace('\\n',\" \")\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "adi_txt_list = get_data(PATH_TO_ADI_TXT, ID_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "adi_title_start = re.compile('\\.T')\n",
        "adi_author_start = re.compile('\\.A')\n",
        "adi_text_start = re.compile('\\.W')\n",
        "\n",
        "adi_txt_data = defaultdict(dict)\n",
        "\n",
        "for line in adi_txt_list:\n",
        "  entries = re.split(adi_title_start,line,1)\n",
        "  id = entries[0].strip()\n",
        "  no_id = entries[1]\n",
        "  if len(re.split(adi_author_start, no_id,1)) > 1:\n",
        "    no_id_entries = re.split(adi_author_start, no_id,1)\n",
        "    adi_txt_data[id]['title'] = no_id_entries[0]\n",
        "    no_title = no_id_entries[1]\n",
        "    no_title_entries = re.split(adi_text_start, no_title)\n",
        "    adi_txt_data[id]['author'] = no_title_entries[0]\n",
        "    adi_txt_data[id]['text'] = no_title_entries[1]\n",
        "  else:\n",
        "    no_id_entries = re.split(adi_text_start, no_id)\n",
        "    adi_txt_data[id]['title'] = no_id_entries[0]\n",
        "    adi_txt_data[id]['text'] = no_id_entries[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hn4N-oo-E8dK"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TAeb_ZObE8dN"
      },
      "source": [
        "Create an index for the adi corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gUqtBwO3E8dQ",
        "colab": {}
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "adi_index = \"adi-corpus\"\n",
        "es.indices.create(adi_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in adi_txt_data.items():\n",
        "  es.index(index=adi_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xRvatFztoo",
        "colab_type": "text"
      },
      "source": [
        "## Cranfield Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3ltU26xDvm2",
        "colab_type": "text"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fS6lVgXAO9F",
        "colab_type": "text"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).  <br>\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/private_experiments/blob/cranfield_corpus/CranfieldCorpus.ipynb) or for parsing in generel read [this guide](https://)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsE0BqpwjrI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "1640201e-2064-43c4-af05-f4fd0650e0b1"
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
        "!tar -xf cran.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variables\n",
        "PATH_TO_CRAN_TXT = '/content/cran.all.1400'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text entries from the text and query files\n",
        "\n",
        "ID_marker = re.compile('\\.I')\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read().replace('\\n',\" \")\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "cran_txt_list = get_data(PATH_TO_CRAN_TXT, ID_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "cran_chunk_start = re.compile('\\.[A,B,T,W]')\n",
        "cran_txt_data = defaultdict(dict)\n",
        "\n",
        "for line in cran_txt_list:\n",
        "  entries= re.split(cran_chunk_start,line)\n",
        "  id = entries[0].strip()\n",
        "  title = entries[1]\n",
        "  author = entries[2]\n",
        "  publication_date = entries[3]\n",
        "  text = entries[4]\n",
        "  cran_txt_data[id]['title'] = title\n",
        "  cran_txt_data[id]['author'] = author\n",
        "  cran_txt_data[id]['publication_date'] = publication_date\n",
        "  cran_txt_data[id]['text'] = text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-16 06:31:35--  http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
            "Resolving ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)... 130.209.240.253\n",
            "Connecting to ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)|130.209.240.253|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 506960 (495K) [application/gzip]\n",
            "Saving to: ‘cran.tar.gz’\n",
            "\n",
            "cran.tar.gz         100%[===================>] 495.08K  1004KB/s    in 0.5s    \n",
            "\n",
            "2020-09-16 06:31:36 (1004 KB/s) - ‘cran.tar.gz’ saved [506960/506960]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n11sJxdEOqk",
        "colab_type": "text"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dex8ehBEXAz"
      },
      "source": [
        "Create an index for the Cranfield corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rs-sHqHZEXA1",
        "colab": {}
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "cran_index = \"cranfield-corpus\"\n",
        "es.indices.create(cran_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in cran_txt_data.items():\n",
        "  es.index(index=cran_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K0ZqWC9JuKbe"
      },
      "source": [
        "## CACM Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIO4lnyXD7dW",
        "colab_type": "text"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eDc4PrF_uKbi"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cacm/).  <br>\n",
        "For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CLroBvrduKbn",
        "colab": {}
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cacm/cacm.tar.gz\n",
        "!tar -xf cacm.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "\n",
        "PATH_TO_CACM_TXT = '/content/cacm.all'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text and query files\n",
        "\n",
        "ID_marker = re.compile('^\\.I',re.MULTILINE)\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read()\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "cacm_txt_list = get_data(PATH_TO_CACM_TXT, ID_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "cacm_chunk_title = re.compile('\\.[T]\\n')\n",
        "cacm_chunk_txt = re.compile('\\n\\.W\\n') # not enough\n",
        "cacm_chunk_txt_pub = re.compile('\\.[W,B]')\n",
        "cacm_chunk_publication = re.compile('\\.[B]\\n')\n",
        "cacm_chunk_author = re.compile('^\\.[A]\\n', re.MULTILINE)\n",
        "cacm_chunk_author_add_cross = re.compile('^\\.[A,N,X]\\n',re.MULTILINE) # not enough\n",
        "cacm_chunk_add_cross = re.compile('\\.[B,N,X]\\n')\n",
        "\n",
        "\n",
        "cacm_txt_data = defaultdict(dict)\n",
        "\n",
        "for line in cacm_txt_list:\n",
        "  entries= re.split(cacm_chunk_title,line)\n",
        "  id = entries[0].strip() #save id\n",
        "  no_id = entries[1]\n",
        "\n",
        "  if len(re.split(cacm_chunk_txt, no_id)) == 2: # is there text\n",
        "    no_id_entries = re.split(cacm_chunk_txt_pub, no_id,1)\n",
        "    cacm_txt_data[id]['title'] = no_id_entries[0].strip() # save title\n",
        "    cacm_txt_data[id]['text'] = no_id_entries[1].strip() # save text\n",
        "    no_title_txt = no_id_entries[1]\n",
        "\n",
        "    if len(re.split(cacm_chunk_author, no_title_txt)) == 2: # is there a auhtor\n",
        "      no_title_entries = re.split(cacm_chunk_author_add_cross, no_title_txt)\n",
        "      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date\n",
        "      cacm_txt_data[id]['author'] = no_title_entries[1].strip() # save athor\n",
        "      cacm_txt_data[id]['add_date'] = no_title_entries[2].strip() # save add date\n",
        "      cacm_txt_data[id]['cross-references'] = no_title_entries[3].strip() # save cross-references\n",
        "\n",
        "    else:\n",
        "      no_title_entries = re.split(cacm_chunk_publication, no_title_txt)\n",
        "      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date\n",
        "      cacm_txt_data[id]['add_date'] = no_title_entries[1].strip() # save add date\n",
        "      cacm_txt_data[id]['cross-references'] = no_title_entries[1].strip() # save cross-references\n",
        "\n",
        "  else:\n",
        "    no_id_entries = re.split(cacm_chunk_publication, no_id,1)\n",
        "    cacm_txt_data[id]['title'] = no_id_entries[0].strip() # save title\n",
        "    no_title = no_id_entries[1]\n",
        "\n",
        "    if len(re.split(cacm_chunk_author, no_title,1)) == 2: # is there a auhtor\n",
        "      no_title_entries = re.split(cacm_chunk_author_add_cross, no_title)\n",
        "      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date\n",
        "      cacm_txt_data[id]['author'] = no_title_entries[1].strip() # save athor\n",
        "      cacm_txt_data[id]['add_date'] = no_title_entries[2].strip() # save add date\n",
        "      cacm_txt_data[id]['cross-references'] = no_title_entries[3].strip() # save cross-references\n",
        "\n",
        "    else:\n",
        "      no_title_entries = re.split(cacm_chunk_add_cross, no_title)\n",
        "      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date\n",
        "      cacm_txt_data[id]['add_date'] = no_title_entries[1].strip() # save add date\n",
        "      cacm_txt_data[id]['cross-references'] = no_title_entries[2].strip() # save cross-references"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTgu3CVzEJ2u",
        "colab_type": "text"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k25W2uvyGwn9"
      },
      "source": [
        "Create an index for the CACM corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaRQfFQdGymV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "cacm_index = \"cacm-corpus\"\n",
        "es.indices.create(cacm_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in cacm_txt_data.items():\n",
        "  es.index(index=cacm_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h46hrLuPNPvc"
      },
      "source": [
        "## CISI Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FtXXPStD6vI",
        "colab_type": "text"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lhEMgc2JNPve"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/).  <br>\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/private_experiments/blob/cisi_corpus/CISICorpus.ipynb) or for parsing in generel read [this guide](https://)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hdn3IdFvNPvi",
        "colab": {}
      },
      "source": [
        "\n",
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
        "!tar -xf cisi.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "PATH_TO_CISI_TXT = '/content/CISI.ALL'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text file\n",
        "\n",
        "ID_marker = re.compile('^\\.I',re.MULTILINE)\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read()\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "cisi_txt_list = get_data(PATH_TO_CISI_TXT, ID_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "cisi_title_start = re.compile('[\\n]\\.T')\n",
        "cisi_author_start = re.compile('[\\n]\\.A')\n",
        "cisi_date_start = re.compile('[\\n]\\.B')\n",
        "cisi_text_start = re.compile('[\\n]\\.W')\n",
        "cisi_cross_start = re.compile('[\\n]\\.X')\n",
        "\n",
        "cisi_txt_data = defaultdict(dict)\n",
        "\n",
        "for line in cisi_txt_list:\n",
        "  entries = re.split(cisi_title_start,line,1)\n",
        "  id = entries[0].strip()#save the id\n",
        "  no_id = entries[1] \n",
        "  \n",
        "  if len(re.split(cisi_author_start, no_id)) >= 2: # is there just one author?\n",
        "    no_id_entries = re.split(cisi_author_start, no_id,1)\n",
        "    cisi_txt_data[id]['title'] = no_id_entries[0].strip() # save title\n",
        "    no_title = no_id_entries[1]\n",
        "\n",
        "    if len(re.split(cisi_date_start, no_title)) > 1: # is there a publication date?\n",
        "      no_title_entries = re.split(cisi_date_start, no_title)\n",
        "      cisi_txt_data[id]['author'] = no_title_entries[0].strip() # save athour\n",
        "      no_author = no_title_entries[1]\n",
        "      no_author_entries = re.split(cisi_text_start, no_author)\n",
        "      cisi_txt_data[id]['publication_date'] = no_author_entries[0].strip() # save publication date\n",
        "      no_author_date = no_author_entries[1]\n",
        "    else:\n",
        "      no_title_entries = re.split(cisi_text_start, no_title)\n",
        "      cisi_txt_data[id]['author'] = no_title_entries[0].strip() # save athour\n",
        "      no_author_date = no_title_entries[1]\n",
        "\n",
        "  else:\n",
        "    no_id_entries = re.split(cisi_author_start, no_id)\n",
        "    cisi_txt_data[id]['title'] = no_id_entries[0].strip() # save title\n",
        "    cisi_txt_data[id]['author'] = no_id_entries[1].strip() # save first author\n",
        "    no_title_entries = re.split(cisi_text_start, no_title)\n",
        "    cisi_txt_data[id]['author'] += ','+no_title_entries[0].strip() # save second athour\n",
        "    no_author_date = no_title_entries[1]\n",
        "\n",
        "  last_entries = re.split(cisi_cross_start, no_author_date)\n",
        "  cisi_txt_data[id]['text'] = last_entries[0].strip() # save text\n",
        "  cisi_txt_data[id]['cross-refrences'] = last_entries[1].strip() # save cross refrences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5SN1aovELJr",
        "colab_type": "text"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uVSqjrHmGnnt"
      },
      "source": [
        "Create an index for the CISI corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slNB44g9GqdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "cisi_index = \"cisi-corpus\"\n",
        "es.indices.create(cisi_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in cisi_txt_data.items():\n",
        "  es.index(index=cisi_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lNYbwKJ5IWa_"
      },
      "source": [
        "## LISA Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fg2D5bCD8NF",
        "colab_type": "text"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "313f1qVFIWbH"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/lisa/).  <br>\n",
        "For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k6NEluV1IWbP",
        "colab": {}
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/lisa/lisa.tar.gz\n",
        "!tar -xf lisa.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "\n",
        "PATH_TO_LISA_TXT = '/content/'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# get the text files\n",
        "\n",
        "file_regex = re.compile('LISA[0-5]')\n",
        "lisa_files = [i for i in os.listdir(PATH_TO_LISA_TXT) if os.path.isfile(os.path.join(PATH_TO_LISA_TXT,i)) and re.match(file_regex,i)]\n",
        "\n",
        "txt_entry_marker = re.compile('\\*{44}',re.MULTILINE)\n",
        "\n",
        "def get_data(PATH_TO_FILES, marker):\n",
        "  \"\"\"\n",
        "  Reads multiple files and spilts text into entries at the entry marker.\n",
        "  The 'marker' contains the regex at which we want to split\n",
        "  Pops last element since it's empty.\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILES,'r') as f:\n",
        "    text = f.read().replace('     ','')\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop()\n",
        "  return lines\n",
        "\n",
        "lisa_txt_list = []\n",
        "for name in lisa_files: \n",
        "  lisa_txt_list.extend(get_data(PATH_TO_LISA_TXT+name, txt_entry_marker))\n",
        "\n",
        "# process text file\n",
        "\n",
        "doc_strip = re.compile('\\n?Document {1,2}')\n",
        "\n",
        "lisa_txt_list_stripped = []\n",
        "lisa_txt_data = defaultdict(dict)\n",
        "\n",
        "for el in lisa_txt_list:\n",
        "  lisa_txt_list_stripped.append(re.sub(doc_strip,'', el))\n",
        "\n",
        "for entry in lisa_txt_list_stripped:\n",
        "  parts = entry.split('\\n')\n",
        "  empty_index = parts.index('')\n",
        "  ID = parts[0]\n",
        "  title = parts[1:empty_index]\n",
        "  text = parts[empty_index+1:]\n",
        "  lisa_txt_data[ID]['title'] = title\n",
        "  lisa_txt_data[ID]['text'] = text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANLR0RmkEIb1",
        "colab_type": "text"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RuX-GdTaHErP"
      },
      "source": [
        "Create an index for the Lisa corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knnX1B-RHGvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "lisa_index = \"lisa-corpus\"\n",
        "es.indices.create(lisa_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in lisa_txt_data.items():\n",
        "  es.index(index=lisa_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LW6zgF8_JG4e"
      },
      "source": [
        "## Medline Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7qvVZxgDzzt",
        "colab_type": "text"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp1yEVO8037d",
        "colab_type": "text"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/med/).  <br>\n",
        "For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/private_experiments/blob/medline_corpus/MedlineCorpus.ipynb) or for parsing in generel read [this guide](https://)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Yo-Qv89JG4q",
        "colab": {}
      },
      "source": [
        "# download and unzip data\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/medl/med.tar.gz\n",
        "!tar -xf med.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variables\n",
        "PATH_TO_MED_TXT = '/content/MED.ALL'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "\n",
        "# get the text and query files\n",
        "\n",
        "ID_marker = re.compile('\\.I')\n",
        "\n",
        "def get_data(PATH_TO_FILE, marker):\n",
        "  \"\"\"\n",
        "  Reads file and spilts text into entries at the ID marker '.I'.\n",
        "  First entry is empty, so it's removed.\n",
        "  'marker' contains the regex at which we want to split\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILE,'r') as f:\n",
        "    text = f.read().replace('\\n',\" \")\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "med_txt_list = get_data(PATH_TO_MED_TXT, ID_marker)\n",
        "\n",
        "# process the text file\n",
        "\n",
        "chunk_start = re.compile('\\.W')\n",
        "med_txt_data = defaultdict(dict)\n",
        "\n",
        "def fill_dictionary(dictionary, chunk_list, marker, key_name):\n",
        "  for n in range(0,len(chunk_list)-1):\n",
        "    line = chunk_list[n+1]\n",
        "    _ , chunk = re.split(marker,line)\n",
        "    dictionary[n+1][key_name] = chunk.strip()\n",
        "\n",
        "fill_dictionary(med_txt_data, med_txt_list, chunk_start, 'text')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpvaXRK_ENmm",
        "colab_type": "text"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p_CEZbwuEs_W"
      },
      "source": [
        "Create an index for the Medline corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fksapREdEs_Y",
        "colab": {}
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "med_index = \"medline-corpus\"\n",
        "es.indices.create(med_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in med_txt_data.items():\n",
        "  es.index(index=med_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EXbsI4B_34QH"
      },
      "source": [
        "## NPL Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W22w4GTsEBu3",
        "colab_type": "text"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N-XV8MnE34QK"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/npl/).  <br>\n",
        "For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FAcUrS6w34QS",
        "colab": {}
      },
      "source": [
        "# download and unzip data\n",
        "\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/npl/npl.tar.gz\n",
        "!tar -xf npl.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "\n",
        "PATH_TO_NPL_TXT = '/content/doc-text'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# get the text, query and rel files\n",
        "\n",
        "txt_entry_marker = re.compile('\\n   /\\n')\n",
        "\n",
        "def get_data(PATH_TO_FILES, marker):\n",
        "  \"\"\"\n",
        "  Reads multiple files and spilts text into entries at the entry marker.\n",
        "  The 'marker' contains the regex at which we want to split\n",
        "  Pops last element since it's empty.\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILES,'r') as f:\n",
        "    text = f.read()\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop()\n",
        "  return lines\n",
        "\n",
        "npl_txt_list = get_data(PATH_TO_NPL_TXT, txt_entry_marker)\n",
        "\n",
        "# process the text file\n",
        "\n",
        "npl_txt_data = defaultdict(dict)\n",
        "\n",
        "for entry in npl_txt_list:\n",
        "  splitted = entry.split('\\n')\n",
        "  splitted = list(filter(None, splitted))\n",
        "  ID = splitted[0]\n",
        "  text = ' '.join(map(str, splitted[1:]))\n",
        "  npl_txt_data[ID]['text'] = text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNEqYeWxEC5W",
        "colab_type": "text"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2SP9glnXHQIl"
      },
      "source": [
        "Create an index for the NPL corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAYR7tGQHRqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "npl_index = \"npl-corpus\"\n",
        "es.indices.create(npl_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in npl_txt_data.items():\n",
        "  es.index(index=npl_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nzHvsKajyd8o"
      },
      "source": [
        "## Time Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsHyfhOHD8xd",
        "colab_type": "text"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z1OwnRY3yd8w"
      },
      "source": [
        "You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/time/).  <br>\n",
        "For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JHfj2Jzbyd83",
        "colab": {}
      },
      "source": [
        "# download and unzip data\n",
        "\n",
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/time/time.tar.gz\n",
        "!tar -xf time.tar.gz\n",
        "\n",
        "# set paths to the dowloaded data as variablesDownload and unzip data.\n",
        "\n",
        "PATH_TO_TIME_TXT = '/content/TIME.ALL'\n",
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import json\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# get the text and query file\n",
        "\n",
        "txt_entry_marker = re.compile('\\*TEXT')\n",
        "\n",
        "def get_data(PATH_TO_FILES, marker):\n",
        "  \"\"\"\n",
        "  Reads multiple files and spilts text into entries at the entry marker.\n",
        "  The 'marker' contains the regex at which we want to split\n",
        "  Pops last element since it's empty.\n",
        "  \"\"\"\n",
        "  with open (PATH_TO_FILES,'r') as f:\n",
        "    text = f.read()\n",
        "    lines = re.split(marker,text)\n",
        "    lines.pop(0)\n",
        "  return lines\n",
        "\n",
        "time_txt_list = get_data(PATH_TO_TIME_TXT, txt_entry_marker)\n",
        "\n",
        "# process text file\n",
        "\n",
        "page_split = re.compile('PAGE \\d{3}')\n",
        "\n",
        "time_txt_data = defaultdict(dict)\n",
        "ID = 1\n",
        "for entry in time_txt_list:\n",
        "  splitted = re.split(page_split,entry)\n",
        "  time_txt_data[ID]['text'] = splitted[1]\n",
        "  ID += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H93yMIMJEG3M",
        "colab_type": "text"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cJ1f_D7WHKcp"
      },
      "source": [
        "Create an index for the Time corpus and index all the documents. This is only possible if it isn't created yet.\n",
        "\n",
        "(For more information see the [Elasticsearch documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPqxAtkkHL4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create index, see https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.create\n",
        "time_index = \"time-corpus\"\n",
        "es.indices.create(time_index)\n",
        "#index documents, see https://elasticsearch-py.readthedocs.io/en/master/#example-usage\n",
        "for ID, doc_data in time_txt_data.items():\n",
        "  es.index(index=time_index, id=ID, body=doc_data)\n",
        "#print new index list\n",
        "create_response = es.cat.indices()\n",
        "print(create_response)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}